{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Импорты и пути к моделям"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from utils import get_name_labse_embs, text_preprocess\n",
    "\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gc\n",
    "from thefuzz import fuzz\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T17:19:40.335596900Z",
     "start_time": "2023-05-30T17:19:18.901960400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "PATH_TO_LABSE = \"./models/LaBSE.pt\"\n",
    "PATH_TO_MULTIMODAL = \"./models/Multi.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T17:19:40.351039200Z",
     "start_time": "2023-05-30T17:19:40.339754100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl.seed_everything(56, workers=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Получаем эмбединги названий LaBSE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "named_data = pd.read_parquet('./datasets/train_data.parquet', columns=[\"variantid\", \"name\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T17:19:42.318968400Z",
     "start_time": "2023-05-30T17:19:40.354016600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   variantid                                               name\n0   51195767  Удлинитель Партнер-Электро ПВС 2х0,75 ГОСТ,6A,...\n1   53565809  Магнитный кабель USB 2.0 A (m) - USB Type-C (m...\n2   56763357  Набор микропрепаратов Konus 25: \"Клетки и ткан...\n3   56961772             Мобильный телефон BQ 1848 Step, черный\n4   61054740  Штатив трипод Tripod 330A для фотоаппаратов, в...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variantid</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>51195767</td>\n      <td>Удлинитель Партнер-Электро ПВС 2х0,75 ГОСТ,6A,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53565809</td>\n      <td>Магнитный кабель USB 2.0 A (m) - USB Type-C (m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56763357</td>\n      <td>Набор микропрепаратов Konus 25: \"Клетки и ткан...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56961772</td>\n      <td>Мобильный телефон BQ 1848 Step, черный</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>61054740</td>\n      <td>Штатив трипод Tripod 330A для фотоаппаратов, в...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T17:19:42.391859200Z",
     "start_time": "2023-05-30T17:19:42.353467400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name_labse_768 = get_name_labse_embs(\"cointegrated/LaBSE-en-ru\", sentences=list(named_data[\"name\"]), device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Получаем эмбединги от LaBSE Tuned"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 96\n",
    "    epochs = 5\n",
    "    lr = 1e-5\n",
    "    lr_warmup_epochs = 5\n",
    "    lr_warmup_decay = 0.01\n",
    "    lr_min = 1e-5\n",
    "\n",
    "args = Args()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ItemsDataset(Dataset):\n",
    "    def __init__(self, pairs, data):\n",
    "        super().__init__()\n",
    "        self.pairs = pairs.values\n",
    "        self.pairs_len = len(self.pairs)\n",
    "\n",
    "        self.names = data['name'].apply(text_preprocess)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, id1, id2 = self.pairs[idx, :]\n",
    "        return (\n",
    "            self.names[id1],\n",
    "            self.names[id2],\n",
    "            target\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LaBSE(pl.LightningModule):\n",
    "    margin = 0.75\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LaBSE, self).__init__()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\n",
    "        self.model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')\n",
    "\n",
    "        self.fc = nn.Linear(768, 768)\n",
    "\n",
    "        #for param in self.model.embeddings.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        #for param in self.model.encoder.parameters():\n",
    "        #    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_input = self.tokenizer(x, padding=True, truncation=True, max_length=256, return_tensors='pt').to('cuda')\n",
    "        model_output = self.model(**encoded_input)\n",
    "\n",
    "        embeddings = torch.nn.functional.normalize(model_output.pooler_output)\n",
    "        embeddings = self.fc(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=0.05\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # self.log('step', batch_idx, logger=True, on_epoch=True)\n",
    "        x1, x2, labels = batch\n",
    "        out1 = self.forward(x1)\n",
    "        out2 = self.forward(x2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        loss = (labels) * torch.pow(dists, 2) + (1 - labels) * torch.pow(torch.clamp(self.margin - dists, min=0.0), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        self.log(\"train_loss\", loss, on_step=False, logger=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1, x2, labels = batch\n",
    "        out1 = self.forward(x1)\n",
    "        out2 = self.forward(x2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        loss = (labels) * torch.pow(dists, 2) + (1 - labels) * torch.pow(torch.clamp(self.margin - dists, min=0.0), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        self.log(\"val_loss\", loss, logger=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(labels.detach().cpu(), 1 - dists.detach().cpu())\n",
    "        except:\n",
    "            auc = 0\n",
    "\n",
    "        self.log(\"val_auc\", auc, logger=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x1, x2, labels = batch\n",
    "        out1 = self.forward(x1)\n",
    "        out2 = self.forward(x2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        return torch.cat([out1, out2, (1 - dists).unsqueeze(-1)], dim=1).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LaBSE()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH_TO_LABSE, map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=False, # CSVLogger('./'),\n",
    "    enable_checkpointing=False,\n",
    "\n",
    "    accelerator='gpu',\n",
    "    devices=[0],\n",
    "    profiler='advanced',\n",
    "    precision=\"16-mixed\",\n",
    "    check_val_every_n_epoch=1,\n",
    "    max_epochs=args.epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pairs = pd.read_parquet('./datasets/test_pairs_wo_target.parquet')\n",
    "test_data = pd.read_parquet('./datasets/test_data.parquet', columns=['variantid', 'name']).set_index('variantid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pairs['target'] = -1\n",
    "test_pairs = test_pairs[['target', 'variantid1', 'variantid2']]\n",
    "test_dataset = ItemsDataset(test_pairs, test_data)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=17,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_features = np.concatenate([pred.numpy() for pred in trainer.predict(model, test_loader)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_embeds = pd.Series(index=test_data.index, dtype='object', name='labse_tuned_768')\n",
    "test_embeds[test_pairs.variantid1] = list(test_features[:, :768])\n",
    "test_embeds[test_pairs.variantid2] = list(test_features[:, 768:768*2])\n",
    "test_embeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Получаем эмбединги от мультимодальной сети"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('./datasets/test_data.parquet').set_index('variantid')\n",
    "test_data['categories'] = test_data['categories'].apply(lambda x: json.loads(x))\n",
    "test_data['main_pic_embeddings_resnet_v1'] = test_data['main_pic_embeddings_resnet_v1'].apply(lambda x: x[0])\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pairs = pd.read_parquet('./datasets/test_pairs_wo_target.parquet')\n",
    "test_pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_cat3 = set()\n",
    "for categories in test_data.categories:\n",
    "    test_cat3.add(categories['3'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors_mapper = {\n",
    " 'ярко-синий': 'ярко-синий',\n",
    " 'ярко-розовый': 'ярко-розовый',\n",
    " 'ярко-зеленый': 'ярко-зеленый',\n",
    " 'ярко-желтый': 'ярко-желтый',\n",
    " 'янтарный': 'янтарный',\n",
    " 'электрик': 'электрик',\n",
    " 'экрю': 'экрю',\n",
    " 'шоколадный': 'шоколадный',\n",
    " 'черный': 'черный',\n",
    " 'черно-синий': 'черно-синий',\n",
    " 'черно-серый': 'черно-серый',\n",
    " 'черно-красный': 'черно-красный',\n",
    " 'черно-зеленый': 'черно-зеленый',\n",
    " 'черн': 'черный',\n",
    " 'чер': 'черный',\n",
    " 'циан': 'бирюзовый',\n",
    " 'цементный': 'цементный',\n",
    " 'хаки': 'хаки',\n",
    " 'фуксия': 'фуксия',\n",
    " 'фисташковый': 'фисташковый',\n",
    " 'фиолетовый': 'фиолетовый',\n",
    " 'фиолетово-синий': 'фиолетово-синий',\n",
    " 'фиолет': 'фиолетовый',\n",
    " 'фиол': 'фиолетовый',\n",
    " 'фиалковый': 'фиалковый',\n",
    " 'тыквенный': 'тыквенный',\n",
    " 'тыква': 'тыквенный',\n",
    " 'травяной': 'травяной',\n",
    " 'томатный': 'томатный',\n",
    " 'тиффани': 'тиффани',\n",
    " 'терракотовый': 'терракотовый',\n",
    " 'терракота': 'терракотовый',\n",
    " 'темно-фиолетовый': 'темно-фиолетовый',\n",
    " 'темно-синий': 'темно-синий',\n",
    " 'темно-серый': 'темно-серый',\n",
    " 'темно-розовый': 'темно-розовый',\n",
    " 'темно-оранжевый': 'темно-оранжевый',\n",
    " 'темно-оливковый': 'темно-оливковый',\n",
    " 'темно-красный': 'темно-красный',\n",
    " 'темно-коричневый': 'темно-коричневый',\n",
    " 'темно-зеленый': 'темно-зеленый',\n",
    " 'темно-голубой': 'темно-голубой',\n",
    " 'темно-бирюзовый': 'темно-бирюзовый',\n",
    " 'темно-бежевый': 'темно-бежевый',\n",
    " 'сливовый': 'сливовый',\n",
    " 'сиреневый': 'сиреневый',\n",
    " 'синий': 'синий',\n",
    " 'сине-зеленый': 'сине-зеленый',\n",
    " 'син': 'синий',\n",
    " 'серый': 'серый',\n",
    " 'серовато-зеленый': 'серовато-зеленый',\n",
    " 'серо-коричневый': 'серо-коричневый',\n",
    " 'серо-зеленый': 'серо-зеленый',\n",
    " 'серо-голубой': 'серо-голубой',\n",
    " 'серо-бежевый': 'серо-бежевый',\n",
    " 'серебряный': 'серебряный',\n",
    " 'серебристый': 'серебристый',\n",
    " 'серебристо-серый': 'серебристо-серый',\n",
    " 'сер': 'серый',\n",
    " 'сепия': 'сепия',\n",
    " 'светло-фиолетовый': 'светло-фиолетовый',\n",
    " 'светло-синий': 'светло-синий',\n",
    " 'светло-серый': 'светло-серый',\n",
    " 'светло-розовый': 'светло-розовый',\n",
    " 'светло-пурпурный': 'светло-пурпурный',\n",
    " 'светло-коричневый': 'светло-коричневый',\n",
    " 'светло-золотистый': 'светло-золотистый',\n",
    " 'светло-зеленый': 'светло-зеленый',\n",
    " 'светло-желтый': 'светло-желтый',\n",
    " 'светло-голубой': 'светло-голубой',\n",
    " 'светло-бирюзовый': 'светло-бирюзовый',\n",
    " 'светло-бежевый': 'светло-бежевый',\n",
    " 'сапфировый': 'сапфировый',\n",
    " 'салатовый': 'салатовый',\n",
    " 'рыжий': 'рыжий',\n",
    " 'розовый': 'розовый',\n",
    " 'розово-фиолетовый': 'розово-фиолетовый',\n",
    " 'розово-золотой': 'розово-золотой',\n",
    " 'разноцветный': 'разноцветный',\n",
    " 'пурпурный': 'пурпурный',\n",
    " 'пурпурно-фиолетовый': 'пурпурно-фиолетовый',\n",
    " 'песочный': 'песочный',\n",
    " 'перу': 'перу',\n",
    " 'персиковый': 'персиковый',\n",
    " 'охра': 'охра',\n",
    " 'орхидея': 'орхидея',\n",
    " 'оранжевый': 'оранжевый',\n",
    " 'оранжево-розовый': 'оранжево-розовый',\n",
    " 'оливковый': 'оливковый',\n",
    " 'огненно-красный': 'огненно-красный',\n",
    " 'нефритовый': 'нефритовый',\n",
    " 'небесный': 'небесный',\n",
    " 'мятный': 'мятный',\n",
    " 'мятно-зеленый': 'мятно-зеленый',\n",
    " 'мята': 'мятный',\n",
    " 'мультиколор': 'мультиколор',\n",
    " 'морковный': 'морковный',\n",
    " 'молочный': 'молочный',\n",
    " 'многоцветный': 'многоцветный',\n",
    " 'медный': 'медный',\n",
    " 'марсала': 'марсала',\n",
    " 'малиновый': 'малиновый',\n",
    " 'малиново-красный': 'малиново-красный',\n",
    " 'малахитовый': 'малахитовый',\n",
    " 'льняной': 'льняной',\n",
    " 'лимонный': 'лимонный',\n",
    " 'лиловый': 'лиловый',\n",
    " 'латунный': 'латунный',\n",
    " 'лаймовый': 'лаймовый',\n",
    " 'лайм': 'лаймовый',\n",
    " 'лазурный': 'лазурный',\n",
    " 'лавандовый': 'лавандовый',\n",
    " 'лаванда': 'лавандовый',\n",
    " 'кремовый': 'кремовый',\n",
    " 'красный': 'красный',\n",
    " 'красновато-коричневый': 'красновато-коричневый',\n",
    " 'красно-оранжевый': 'красно-оранжевый',\n",
    " 'красно-коричневый': 'красно-коричневый',\n",
    " 'красн': 'красный',\n",
    " 'крас': 'красный',\n",
    " 'кофейный': 'кофейный',\n",
    " 'космос': 'космос',\n",
    " 'коричневый': 'коричневый',\n",
    " 'коричнево-красный': 'коричнево-красный',\n",
    " 'коричнево-бежевый': 'коричнево-бежевый',\n",
    " 'коралловый': 'коралловый',\n",
    " 'кораллово-красный': 'кораллово-красный',\n",
    " 'кобальтовый': 'кобальтовый',\n",
    " 'кирпичный': 'кирпичный',\n",
    " 'кирпично-красный': 'кирпично-красный',\n",
    " 'кварцевый': 'кварцевый',\n",
    " 'кардинал': 'кардинал',\n",
    " 'канареечный': 'канареечный',\n",
    " 'камуфляжный': 'камуфляжный',\n",
    " 'индиго': 'индиго',\n",
    " 'изумрудный': 'изумрудный',\n",
    " 'изумрудно-зеленый': 'изумрудно-зеленый',\n",
    " 'изумруд': 'изумрудный',\n",
    " 'золотой': 'золотой',\n",
    " 'золотистый': 'золотистый',\n",
    " 'зеленый': 'зеленый',\n",
    " 'зелено-серый': 'зелено-серый',\n",
    " 'зел': 'зеленый',\n",
    " 'жемчужно-белый': 'жемчужно-белый',\n",
    " 'желтый': 'желтый',\n",
    " 'желто-розовый': 'желто-розовый',\n",
    " 'желто-зеленый': 'желто-зеленый',\n",
    " 'желт': 'желтый',\n",
    " 'гусеница': 'гусеница',\n",
    " 'грушевый': 'грушевый',\n",
    " 'графит': 'графит',\n",
    " 'гранитный': 'гранитный',\n",
    " 'гранатовый': 'гранатовый',\n",
    " 'горчичный': 'горчичный',\n",
    " 'голубой': 'голубой',\n",
    " 'голуб': 'голубой',\n",
    " 'глициния': 'глициния',\n",
    " 'вишня': 'вишневый',\n",
    " 'вишневый': 'вишневый',\n",
    " 'васильковый': 'васильковый',\n",
    " 'ванильный': 'ванильный',\n",
    " 'бурый': 'бурый',\n",
    " 'бронзовый': 'бронзовый',\n",
    " 'бордовый': 'бордовый',\n",
    " 'бордо': 'бордовый',\n",
    " 'болотный': 'болотный',\n",
    " 'бледно-розовый': 'бледно-розовый',\n",
    " 'бледно-пурпурный': 'бледно-пурпурный',\n",
    " 'бледно-желтый': 'бледно-желтый',\n",
    " 'бирюзовый': 'бирюзовый',\n",
    " 'бирюзово-зеленый': 'бирюзово-зеленый',\n",
    " 'белый': 'белый',\n",
    " 'белоснежный': 'белоснежный',\n",
    " 'бело-зеленый': 'бело-зеленый',\n",
    " 'бел': 'белый',\n",
    " 'бежевый': 'бежевый',\n",
    " 'бежево-серый': 'бежево-серый',\n",
    " 'бежево-розовый': 'бежево-розовый',\n",
    " 'баклажановый': 'баклажановый',\n",
    " 'антрацитовый': 'антрацитовый',\n",
    " 'аметистовый': 'аметистовый',\n",
    " 'алый': 'алый',\n",
    " 'аквамариновый': 'аквамариновый',\n",
    " 'аква': 'аква',\n",
    " 'абрикосовый': 'абрикосовый',\n",
    " 'yellow': 'желтый',\n",
    " 'wine': 'wine',\n",
    " 'white': 'белый',\n",
    " 'violet': 'фиолетовый',\n",
    " 'vanilla': 'ванильный',\n",
    " 'ultramarine': 'ultramarine',\n",
    " 'turquoise': 'бирюзовый',\n",
    " 'tomato': 'томатный',\n",
    " 'teal': 'teal',\n",
    " 'tan': 'tan',\n",
    " 'snow': 'snow',\n",
    " 'silver': 'серебряный',\n",
    " 'sapphire': 'сапфировый',\n",
    " 'red': 'красный',\n",
    " 'purple': 'фиолетовый',\n",
    " 'pink': 'розовый',\n",
    " 'peru': 'перу',\n",
    " 'pear': 'грушевый',\n",
    " 'peach': 'персиковый',\n",
    " 'orchid': 'орхидея',\n",
    " 'orange': 'оранжевый',\n",
    " 'olive': 'оливковый',\n",
    " 'navy': 'navy',\n",
    " 'magenta': 'пурпурный',\n",
    " 'linen': 'linen',\n",
    " 'lime': 'лаймовый',\n",
    " 'lilac': 'сиреневый',\n",
    " 'lemon': 'lemon',\n",
    " 'lavender': 'лавандовый',\n",
    " 'khaki': 'хаки',\n",
    " 'jade': 'нефритовый',\n",
    " 'ivory': 'ivory',\n",
    " 'indigo': 'индиго',\n",
    " 'grey': 'серый',\n",
    " 'green': 'зеленый',\n",
    " 'gray': 'серый',\n",
    " 'gold': 'золотой',\n",
    " 'fuchsia': 'фуксия',\n",
    " 'flax': 'flax',\n",
    " 'emerald': 'emerald',\n",
    " 'denim': 'denim',\n",
    " 'cyan': 'бирюзовый',\n",
    " 'cream': 'кремовый',\n",
    " 'corn': 'corn',\n",
    " 'coral': 'коралловый',\n",
    " 'copper': 'медный',\n",
    " 'cobalt': 'кобальтовый',\n",
    " 'chocolate': 'шоколадный',\n",
    " 'burgundy': 'бордовый',\n",
    " 'buff': 'buff',\n",
    " 'brown': 'коричневый',\n",
    " 'bronze': 'бронзовый',\n",
    " 'brass': 'латунный',\n",
    " 'blue': 'голубой',\n",
    " 'blond': 'blond',\n",
    " 'black': 'черный',\n",
    " 'beige': 'бежевый',\n",
    " 'azure': 'лазурный',\n",
    " 'aquamarine': 'аквамариновый',\n",
    " 'aqua': 'аквамариновый',\n",
    " 'amethyst': 'аметистовый',\n",
    " 'amber': 'янтарный'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color_vocab = {}\n",
    "for color, v in colors_mapper.items():\n",
    "    color_vocab[v] = len(color_vocab) + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 96\n",
    "    epochs = 10\n",
    "    lr = 1e-5\n",
    "\n",
    "args = Args()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ItemsDataset(Dataset):\n",
    "    def __init__(self, pairs, data):\n",
    "        super().__init__()\n",
    "        self.pairs = pairs.values\n",
    "        self.pairs_len = len(self.pairs)\n",
    "\n",
    "        self.main_pic_embs = data['main_pic_embeddings_resnet_v1']\n",
    "\n",
    "        categories = data['categories'].copy().apply(lambda x: x['3'])\n",
    "        categories[~categories.isin(categories_map)] = 'rest'\n",
    "        self.categories = categories.apply(lambda v: categories_map[v])\n",
    "\n",
    "        def color_to_idx(colors):\n",
    "            if colors is None:\n",
    "                return []\n",
    "            return [color_vocab[colors_mapper[color]] for color in colors]\n",
    "        def drop_dup_colors(colors):\n",
    "            if colors is None:\n",
    "                return []\n",
    "            res = []\n",
    "            for v in colors:\n",
    "                if v not in res:\n",
    "                    res.append(v)\n",
    "            return res\n",
    "        colors = data['color_parsed'].copy().apply(color_to_idx).apply(drop_dup_colors)\n",
    "        def pad_colors(colors):\n",
    "            max_colors = 17\n",
    "            if len(colors) > max_colors:\n",
    "                return colors[:max_colors]\n",
    "            return colors + [0] * (max_colors - len(colors))\n",
    "        self.colors = colors.apply(pad_colors)\n",
    "\n",
    "        self.names = data['name'].apply(text_preprocess)\n",
    "\n",
    "        self.name_bert_embs = data['name_bert_64']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, id1, id2 = self.pairs[idx, :]\n",
    "        return (\n",
    "            self.categories[id1],\n",
    "            torch.tensor(self.colors[id1]),\n",
    "            self.names[id1],\n",
    "            torch.tensor(self.main_pic_embs[id1]),\n",
    "            torch.tensor(self.name_bert_embs[id1]),\n",
    "\n",
    "            self.categories[id2],\n",
    "            torch.tensor(self.colors[id2]),\n",
    "            self.names[id2],\n",
    "            self.main_pic_embs[id2],\n",
    "            torch.tensor(self.name_bert_embs[id2]),\n",
    "\n",
    "            target\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiModalNet(pl.LightningModule):\n",
    "    margin = 0.75\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "\n",
    "        # attrs\n",
    "        self.category_embedding = nn.Embedding(\n",
    "            num_embeddings=len(categories_map),\n",
    "            embedding_dim=len(categories_map) // 2,\n",
    "            padding_idx=None\n",
    "        )\n",
    "\n",
    "        self.color_embedding = nn.Embedding(\n",
    "            num_embeddings=len(color_vocab) + 2,\n",
    "            embedding_dim=(len(color_vocab) + 2) // 2,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.color_lstm_hidden = 64\n",
    "        self.color_lstm = nn.LSTM(\n",
    "            input_size=(len(color_vocab) + 2) // 2,\n",
    "            hidden_size=self.color_lstm_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # name\n",
    "        self.LaBSE_tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\n",
    "        self.LaBSE_model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')\n",
    "        self.LaBSE_fc = nn.Linear(768, 768)\n",
    "\n",
    "        # net\n",
    "        input_size = len(categories_map) // 2 + 2*self.color_lstm_hidden + 768 + 128 + 64\n",
    "        output_size = 768\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        self.embedding_dropout = nn.Dropout(p=0.05)\n",
    "\n",
    "        deberta_cfg = DebertaV2Config(\n",
    "            hidden_size=input_size,\n",
    "            num_hidden_layers=1,\n",
    "            num_attention_heads=1,\n",
    "            intermediate_size=1024,\n",
    "        )\n",
    "        self.deberta = DebertaV2Model(deberta_cfg, ).encoder\n",
    "\n",
    "        features_num = 2 * input_size\n",
    "        embedding_size = (features_num + output_size) // 2\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.BatchNorm1d(features_num),\n",
    "            nn.Linear(features_num, embedding_size, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "            nn.Linear(embedding_size, embedding_size, bias=False),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_size, output_size)\n",
    "\n",
    "    def forward(self, categories, colors, names, pic_embs, name_bert_embs):\n",
    "        categories_output = self.category_embedding(categories)\n",
    "\n",
    "        colors_emb = self.color_embedding(colors)\n",
    "        output, (ht, ct) = self.color_lstm(colors_emb)\n",
    "        out_forward = output[:, -1, :self.color_lstm_hidden]\n",
    "        out_reverse = output[:, 0, self.color_lstm_hidden:]\n",
    "        colors_output = torch.cat([out_forward, out_reverse], 1)\n",
    "\n",
    "        encoded_input = self.LaBSE_tokenizer(\n",
    "            names, padding=True, truncation=True, max_length=256, return_tensors='pt'\n",
    "        ).to('cuda')\n",
    "        model_output = self.LaBSE_model(**encoded_input)\n",
    "        embeddings = torch.nn.functional.normalize(model_output.pooler_output)\n",
    "        names_output = self.LaBSE_fc(embeddings)\n",
    "\n",
    "        pics_output = torch.nn.functional.normalize(pic_embs)\n",
    "\n",
    "        names_bert_output = torch.nn.functional.normalize(name_bert_embs)\n",
    "\n",
    "        x = torch.cat([categories_output, colors_output, names_output, pics_output, names_bert_output], dim=1)\n",
    "        x = self.bn(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        attention_mask = torch.ones((x.shape[0], 1), device='cuda')\n",
    "        last_hidden = self.deberta(x, attention_mask)\n",
    "        last_hidden = torch.concat([last_hidden[0].mean(1), last_hidden[0].max(1)[0]], -1)\n",
    "        outputs = self.neck(last_hidden)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        outputs = torch.nn.functional.normalize(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=0.05\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # self.log('step', batch_idx, logger=True, on_epoch=True)\n",
    "        categories1, colors1, names1, pic_embs1, name_bert_embs1,\\\n",
    "        categories2, colors2, names2, pic_embs2, name_bert_embs2,\\\n",
    "        labels = batch\n",
    "        out1 = self.forward(categories1, colors1, names1, pic_embs1, name_bert_embs1)\n",
    "        out2 = self.forward(categories2, colors2, names2, pic_embs2, name_bert_embs2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        loss = (labels) * torch.pow(dists, 2) + (1 - labels) * torch.pow(torch.clamp(self.margin - dists, min=0.0), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        self.log(\"train_loss\", loss, on_step=False, logger=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        categories1, colors1, names1, pic_embs1, name_bert_embs1,\\\n",
    "        categories2, colors2, names2, pic_embs2, name_bert_embs2,\\\n",
    "        labels = batch\n",
    "        out1 = self.forward(categories1, colors1, names1, pic_embs1, name_bert_embs1)\n",
    "        out2 = self.forward(categories2, colors2, names2, pic_embs2, name_bert_embs2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        loss = (labels) * torch.pow(dists, 2) + (1 - labels) * torch.pow(torch.clamp(self.margin - dists, min=0.0), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        self.log(\"val_loss\", loss, logger=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(labels.detach().cpu(), 1 - dists.detach().cpu())\n",
    "        except:\n",
    "            auc = 0\n",
    "\n",
    "        self.log(\"val_auc\", auc, logger=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        categories1, colors1, names1, pic_embs1, name_bert_embs1,\\\n",
    "        categories2, colors2, names2, pic_embs2, name_bert_embs2,\\\n",
    "        labels = batch\n",
    "        out1 = self.forward(categories1, colors1, names1, pic_embs1, name_bert_embs1)\n",
    "        out2 = self.forward(categories2, colors2, names2, pic_embs2, name_bert_embs2)\n",
    "\n",
    "        dists = nn.PairwiseDistance()(out1, out2)\n",
    "        return torch.cat([out1, out2, (1 - dists).unsqueeze(-1)], dim=1).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MultiModalNet()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath='./MultiModal/', filename='products-{epoch:02d}-{val_auc:.4f}-normalize', monitor='val_auc', mode='max'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False, # CSVLogger('./'),\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    accelerator='gpu',\n",
    "    devices=[0],\n",
    "    profiler='advanced',\n",
    "    precision=\"16-mixed\",\n",
    "    check_val_every_n_epoch=1,\n",
    "    max_epochs=args.epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pairs['target'] = -1\n",
    "test_pairs = test_pairs[['target', 'variantid1', 'variantid2']]\n",
    "test_dataset = ItemsDataset(test_pairs, test_data)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_features = np.concatenate([pred.numpy() for pred in trainer.predict(model, test_loader)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_embeds = pd.Series(index=test_data.index, dtype='object', name='multimodal_tuned_768')\n",
    "test_embeds[test_pairs.variantid1] = list(test_features[:, :768])\n",
    "test_embeds[test_pairs.variantid2] = list(test_features[:, 768:768*2])\n",
    "test_embeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
